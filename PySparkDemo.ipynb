{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# pip install Pyspark\n",
    "# PySpark\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/\n",
    "#https://www.guru99.com/pyspark-tutorial.html#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"spark demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-air:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10c471a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(firstname='James', middlename='', lastname='Smith', dob='1991-04-01', gender='M', salary=3000), Row(firstname='Michael', middlename='Rose', lastname='', dob='2000-05-19', gender='M', salary=4000), Row(firstname='Robert', middlename='', lastname='Williams', dob='1978-09-05', gender='M', salary=4000), Row(firstname='Maria', middlename='Anne', lastname='Jones', dob='1967-12-01', gender='F', salary=4000), Row(firstname='Jen', middlename='Mary', lastname='Brown', dob='1980-02-17', gender='F', salary=-1)]\n",
      "------------------\n",
      "James,1991-04-01\n",
      "Michael,2000-05-19\n",
      "Robert,1978-09-05\n",
      "Maria,1967-12-01\n",
      "Jen,1980-02-17\n"
     ]
    }
   ],
   "source": [
    "dataCollect = df.collect()\n",
    "print(dataCollect)\n",
    "\n",
    "print(\"------------------\")\n",
    "for row in dataCollect:\n",
    "    print(row['firstname'] + \",\" +str(row['dob']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|firstname|dob       |\n",
      "+---------+----------+\n",
      "|James    |1991-04-01|\n",
      "|Michael  |2000-05-19|\n",
      "|Robert   |1978-09-05|\n",
      "|Maria    |1967-12-01|\n",
      "|Jen      |1980-02-17|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select \n",
    "\n",
    "dataSelect = df.select(\"firstname\",\"dob\")\n",
    "dataSelect.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df_country = spark.createDataFrame(data = data, schema = columns)\n",
    "df_country.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|firstname|lastname|country|state|\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |Michael  |Rose    |USA    |NY   |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |Maria    |Jones   |USA    |FL   |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |Robert   |Williams|USA    |CA   |\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join \n",
    "\n",
    "df_join = df.join(df_country,df.firstname == df_country.firstname,'inner')\n",
    "df_join.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|firstname|lastname|country|state|\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |James    |Smith   |USA    |CA   |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |null     |null    |null   |null |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |Michael  |Rose    |USA    |NY   |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |Maria    |Jones   |USA    |FL   |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |Robert   |Williams|USA    |CA   |\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#left Join\n",
    "df_join = df.join(df_country,df.firstname == df_country.firstname,'left')\n",
    "df_join.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|firstname|lastname|country|state|\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |James    |Smith   |USA    |CA   |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |null     |null    |null   |null |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |Michael  |Rose    |USA    |NY   |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |Maria    |Jones   |USA    |FL   |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |Robert   |Williams|USA    |CA   |\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#outer Join\n",
    "df_join = df.join(df_country,df.firstname == df_country.firstname,'outer')\n",
    "df_join.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|firstname|lastname|country|state|\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |Michael  |Rose    |USA    |NY   |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |Maria    |Jones   |USA    |FL   |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |Robert   |Williams|USA    |CA   |\n",
      "+---------+----------+--------+----------+------+------+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#right Join\n",
    "df_join = df.join(df_country,df.firstname == df_country.firstname,'right')\n",
    "df_join.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991\n"
     ]
    }
   ],
   "source": [
    "# dateConvertionFunction\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def yearFromDate(dateStr):\n",
    "    dt = datetime.strptime(dateStr, '%Y-%m-%d')\n",
    "    return dt.year\n",
    "\n",
    "print(yearFromDate('1991-04-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF Creation\n",
    "from pyspark.sql.functions import col, udf,array_contains\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "#Converting function to UDF StringType() is by default hence not required \n",
    "yearFromDF = udf(lambda z: yearFromDate(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year|\n",
      "+---------+----------+--------+----------+------+------+----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991|\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |2000|\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |1978|\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |1967|\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |1980|\n",
      "+---------+----------+--------+----------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('year',yearFromDF(col('dob')))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating a UDF is a 2 step process, first, you need to create a Python function, \n",
    "#second convert function to UDF using SQL udf() function, however, you can avoid these two steps and create it with just a single step by using annotations\n",
    "\n",
    "@udf(returnType=StringType()) \n",
    "def monthOfDate(dateStr):\n",
    "    dt = datetime.strptime(dateStr, '%Y-%m-%d')\n",
    "    return dt.month\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year|month|\n",
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991|4    |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |2000|5    |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |1978|9    |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |1967|12   |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |1980|2    |\n",
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('month',monthOfDate(col('dob')))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year_of_dod|month|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991       |4    |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |2000       |5    |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |1978       |9    |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |1967       |12   |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |1980       |2    |\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('year','year_of_dod')\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(firstname='James', middlename='', lastname='Smith', dob='1991-04-01', gender='M', salary=3000, year_of_dod='1991', month='4'), Row(firstname='Michael', middlename='Rose', lastname='', dob='2000-05-19', gender='M', salary=4000, year_of_dod='2000', month='5'), Row(firstname='Robert', middlename='', lastname='Williams', dob='1978-09-05', gender='M', salary=4000, year_of_dod='1978', month='9'), Row(firstname='Maria', middlename='Anne', lastname='Jones', dob='1967-12-01', gender='F', salary=4000, year_of_dod='1967', month='12'), Row(firstname='Jen', middlename='Mary', lastname='Brown', dob='1980-02-17', gender='F', salary=-1, year_of_dod='1980', month='2')]\n"
     ]
    }
   ],
   "source": [
    "dataCollect = df.collect()\n",
    "print(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Smith,1991-04-01\n",
      "Michael,,2000-05-19\n",
      "Robert,Williams,1978-09-05\n",
      "Maria,Jones,1967-12-01\n",
      "Jen,Brown,1980-02-17\n"
     ]
    }
   ],
   "source": [
    "for row in dataCollect:\n",
    "    print(row['firstname'] + \",\" +str(row['lastname'])+\",\" +str(row['dob']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|year_of_dod|month|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       1991|    4|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter or where (both are same)\n",
    "df.filter(df.year_of_dod == '1991').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|year_of_dod|month|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       1991|    4|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.year_of_dod == '1991').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|year_of_dod|month|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       1991|    4|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|       1980|    2|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple filter conditions\n",
    "df.filter((df.year_of_dod == '1991') | (df.year_of_dod == '1980')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|year_of_dod|month|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       1991|    4|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|       1980|    2|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((df.year_of_dod == '1991') | (df.year_of_dod == '1980')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year_of_dod|month|\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991       |4    |\n",
      "+---------+----------+--------+----------+------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('lastname').like(\"%Smi%\")).show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jen          |Finance   |4900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Duplicates and Distinct\n",
    "\n",
    "duplicateDataExp = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jen\", \"Finance\", 4900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "columnsForDropDup= [\"employee_name\", \"department\", \"salary\"]\n",
    "dfExampleForDuplicates = spark.createDataFrame(data = duplicateDataExp, schema = columnsForDropDup)\n",
    "dfExampleForDuplicates.printSchema()\n",
    "dfExampleForDuplicates.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Scott|   Finance|  3300|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfExampleForDuplicates.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Scott|   Finance|  3300|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfExampleForDuplicates.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        Scott|   Finance|  3300|\n",
      "|        James|     Sales|  3000|\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfExampleForDuplicates.dropDuplicates([\"employee_name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OrderBy or Sort\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "dfForSort = spark.createDataFrame(data = simpleData,schema = columns)\n",
    "dfForSort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OrderBy and Sort By both are same\n",
    "\n",
    "dfForSort.sort(col(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.orderBy(col(\"salary\").desc()).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForSort.createOrReplaceTempView(\"EMPLOYEE_SALARY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from EMPLOYEE_SALARY order by salary desc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from EMPLOYEE_SALARY order by salary asc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy  = always it will return the Grouped Data we need to apply actions on it like count,sum,Min,Max,Agg etc\n",
    "\n",
    "dfForSort.groupBy(col(\"department\")).sum(\"salary\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.groupBy(col('department'),col('state')).sum('salary','bonus').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n",
      "+----------+-----+-----------+----------+----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|max(bonus)|min(bonus)|\n",
      "+----------+-----+-----------+----------+----------+----------+\n",
      "|Finance   |NY   |162000     |34000     |19000     |15000     |\n",
      "|Marketing |NY   |91000      |21000     |21000     |21000     |\n",
      "|Sales     |CA   |81000      |23000     |23000     |23000     |\n",
      "|Marketing |CA   |80000      |18000     |18000     |18000     |\n",
      "|Finance   |CA   |189000     |47000     |24000     |23000     |\n",
      "|Sales     |NY   |176000     |30000     |20000     |10000     |\n",
      "+----------+-----+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select department,state,sum(salary),sum(bonus) from EMPLOYEE_SALARY group by department,state\").show(truncate = False)\n",
    "\n",
    "spark.sql(\"select department,state,sum(salary),sum(bonus),max(bonus),min(bonus) from EMPLOYEE_SALARY group by department,state\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|collect_list(salary)                                           |\n",
      "+---------------------------------------------------------------+\n",
      "|[90000, 86000, 81000, 90000, 99000, 83000, 79000, 80000, 91000]|\n",
      "+---------------------------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------------+\n",
      "|collect_list(salary)                                           |\n",
      "+---------------------------------------------------------------+\n",
      "|[90000, 86000, 81000, 90000, 99000, 83000, 79000, 80000, 91000]|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "\n",
    "dfForSort.select(collect_list(\"salary\")).show(truncate=False)\n",
    "#listagg in sql\n",
    "spark.sql(\"select collect_list(salary) from EMPLOYEE_SALARY\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------+\n",
      "|department|collect_list(salary)        |\n",
      "+----------+----------------------------+\n",
      "|Sales     |[90000, 86000, 81000]       |\n",
      "|Finance   |[90000, 99000, 83000, 79000]|\n",
      "|Marketing |[80000, 91000]              |\n",
      "+----------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select department,collect_list(salary) from EMPLOYEE_SALARY group by department\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------+\n",
      "|department|collect_data                |\n",
      "+----------+----------------------------+\n",
      "|Sales     |[90000, 86000, 81000]       |\n",
      "|Finance   |[90000, 99000, 83000, 79000]|\n",
      "|Marketing |[80000, 91000]              |\n",
      "+----------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Temp= dfForSort.groupby(\"department\").agg(collect_list(\"salary\").alias(\"collect_data\"))\n",
    "df_Temp.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|col  |\n",
      "+----------+-----+\n",
      "|Sales     |90000|\n",
      "|Sales     |86000|\n",
      "|Sales     |81000|\n",
      "|Finance   |90000|\n",
      "|Finance   |99000|\n",
      "|Finance   |83000|\n",
      "|Finance   |79000|\n",
      "|Marketing |80000|\n",
      "|Marketing |91000|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df_Temp.select(df_Temp.department,explode(df_Temp.collect_data)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+\n",
      "|department|CA    |NY    |\n",
      "+----------+------+------+\n",
      "|Sales     |81000 |176000|\n",
      "|Finance   |189000|162000|\n",
      "|Marketing |80000 |91000 |\n",
      "+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot Table\n",
    "#Spark pivot() function is used to pivot/rotate the data from one DataFrame column into multiple columns \n",
    "#(transform rows to columns) and unpivot is used to transform it back (transform columns to rows).\n",
    "dfForSort.groupBy(\"department\").pivot(\"state\").sum(\"salary\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+------+\n",
      "|state|Finance|Marketing|Sales |\n",
      "+-----+-------+---------+------+\n",
      "|CA   |189000 |80000    |81000 |\n",
      "|NY   |162000 |91000    |176000|\n",
      "+-----+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.groupBy(\"state\").pivot(\"department\").sum(\"salary\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+----------+\n",
      "|employee_name|department|state|salary|age|bonus|row_number|\n",
      "+-------------+----------+-----+------+---+-----+----------+\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|1         |\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|2         |\n",
      "|James        |Sales     |NY   |90000 |34 |10000|3         |\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|1         |\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|2         |\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|3         |\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|4         |\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|1         |\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|2         |\n",
      "+-------------+----------+-----+------+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark Window Functions\n",
    "#PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row.\n",
    "\n",
    "#row_number Window Function\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,rank\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "dfForSort.withColumn('row_number',row_number().over(windowSpec)).show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+----+\n",
      "|employee_name|department|state|salary|age|bonus|rank|\n",
      "+-------------+----------+-----+------+---+-----+----+\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|1   |\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|1   |\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|1   |\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|2   |\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|2   |\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|2   |\n",
      "|James        |Sales     |NY   |90000 |34 |10000|3   |\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|3   |\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|4   |\n",
      "+-------------+----------+-----+------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.withColumn('rank',rank().over(windowSpec)).orderBy('rank').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+-----+\n",
      "|employee_name|department|state|salary|  lag|\n",
      "+-------------+----------+-----+------+-----+\n",
      "|       Robert|     Sales|   CA| 81000| null|\n",
      "|      Michael|     Sales|   NY| 86000|81000|\n",
      "|        James|     Sales|   NY| 90000|86000|\n",
      "|          Jen|   Finance|   NY| 79000| null|\n",
      "|        Scott|   Finance|   NY| 83000|79000|\n",
      "|        Maria|   Finance|   CA| 90000|83000|\n",
      "|        Raman|   Finance|   CA| 99000|90000|\n",
      "|         Jeff| Marketing|   CA| 80000| null|\n",
      "|        Kumar| Marketing|   NY| 91000|80000|\n",
      "+-------------+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lag\"\"\"\n",
    "from pyspark.sql.functions import lag    \n",
    "dfForSort.withColumn(\"lag\",lag(\"salary\",1).over(windowSpec)).select('employee_name','department','state','salary','lag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+-----+\n",
      "|employee_name|department|state|salary|  lag|\n",
      "+-------------+----------+-----+------+-----+\n",
      "|       Robert|     Sales|   CA| 81000| null|\n",
      "|      Michael|     Sales|   NY| 86000| null|\n",
      "|        James|     Sales|   NY| 90000|81000|\n",
      "|          Jen|   Finance|   NY| 79000| null|\n",
      "|        Scott|   Finance|   NY| 83000| null|\n",
      "|        Maria|   Finance|   CA| 90000|79000|\n",
      "|        Raman|   Finance|   CA| 99000|83000|\n",
      "|         Jeff| Marketing|   CA| 80000| null|\n",
      "|        Kumar| Marketing|   NY| 91000| null|\n",
      "+-------------+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)).select('employee_name','department','state','salary','lag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+-----+\n",
      "|employee_name|department|state|salary| lead|\n",
      "+-------------+----------+-----+------+-----+\n",
      "|       Robert|     Sales|   CA| 81000|86000|\n",
      "|      Michael|     Sales|   NY| 86000|90000|\n",
      "|        James|     Sales|   NY| 90000| null|\n",
      "|          Jen|   Finance|   NY| 79000|83000|\n",
      "|        Scott|   Finance|   NY| 83000|90000|\n",
      "|        Maria|   Finance|   CA| 90000|99000|\n",
      "|        Raman|   Finance|   CA| 99000| null|\n",
      "|         Jeff| Marketing|   CA| 80000|91000|\n",
      "|        Kumar| Marketing|   NY| 91000| null|\n",
      "+-------------+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #\"\"\"lead\"\"\"\n",
    "from pyspark.sql.functions import lead    \n",
    "dfForSort.withColumn(\"lead\",lead(\"salary\",1).over(windowSpec)).select('employee_name','department','state','salary','lead').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+-----+\n",
      "|employee_name|department|state|salary| lead|\n",
      "+-------------+----------+-----+------+-----+\n",
      "|       Robert|     Sales|   CA| 81000|90000|\n",
      "|      Michael|     Sales|   NY| 86000| null|\n",
      "|        James|     Sales|   NY| 90000| null|\n",
      "|          Jen|   Finance|   NY| 79000|90000|\n",
      "|        Scott|   Finance|   NY| 83000|99000|\n",
      "|        Maria|   Finance|   CA| 90000| null|\n",
      "|        Raman|   Finance|   CA| 99000| null|\n",
      "|         Jeff| Marketing|   CA| 80000| null|\n",
      "|        Kumar| Marketing|   NY| 91000| null|\n",
      "+-------------+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).select('employee_name','department','state','salary','lead').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+-----+\n",
      "|employee_name|department|state|salary| lead|\n",
      "+-------------+----------+-----+------+-----+\n",
      "|       Robert|     Sales|   CA| 81000|81000|\n",
      "|      Michael|     Sales|   NY| 86000|86000|\n",
      "|        James|     Sales|   NY| 90000|90000|\n",
      "|          Jen|   Finance|   NY| 79000|79000|\n",
      "|        Scott|   Finance|   NY| 83000|83000|\n",
      "|        Maria|   Finance|   CA| 90000|90000|\n",
      "|        Raman|   Finance|   CA| 99000|99000|\n",
      "|         Jeff| Marketing|   CA| 80000|80000|\n",
      "|        Kumar| Marketing|   NY| 91000|91000|\n",
      "+-------------+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfForSort.withColumn(\"lead\",lead(\"salary\",0).over(windowSpec)).select('employee_name','department','state','salary','lead').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+------+\n",
      "|employee_name|department|state|salary|sum   |\n",
      "+-------------+----------+-----+------+------+\n",
      "|James        |Sales     |NY   |90000 |257000|\n",
      "|Michael      |Sales     |NY   |86000 |257000|\n",
      "|Robert       |Sales     |CA   |81000 |257000|\n",
      "|Maria        |Finance   |CA   |90000 |351000|\n",
      "|Raman        |Finance   |CA   |99000 |351000|\n",
      "|Scott        |Finance   |NY   |83000 |351000|\n",
      "|Jen          |Finance   |NY   |79000 |351000|\n",
      "|Jeff         |Marketing |CA   |80000 |171000|\n",
      "|Kumar        |Marketing |NY   |91000 |171000|\n",
      "+-------------+----------+-----+------+------+\n",
      "\n",
      "+-------------+----------+-----+------+-----+\n",
      "|employee_name|department|state|salary|max  |\n",
      "+-------------+----------+-----+------+-----+\n",
      "|James        |Sales     |NY   |90000 |90000|\n",
      "|Michael      |Sales     |NY   |86000 |90000|\n",
      "|Robert       |Sales     |CA   |81000 |90000|\n",
      "|Maria        |Finance   |CA   |90000 |99000|\n",
      "|Raman        |Finance   |CA   |99000 |99000|\n",
      "|Scott        |Finance   |NY   |83000 |99000|\n",
      "|Jen          |Finance   |NY   |79000 |99000|\n",
      "|Jeff         |Marketing |CA   |80000 |91000|\n",
      "|Kumar        |Marketing |NY   |91000 |91000|\n",
      "+-------------+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# windows aggration function \n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "dfForSort.withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)).select('employee_name','department','state','salary','sum').show(truncate = False)\n",
    "dfForSort.withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)).select('employee_name','department','state','salary','max').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------------+------+------------+---------------+------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "|x  |age|workclass       |fnlwgt|education   |educational-num|marital-status    |occupation       |relationship |race              |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---+----------------+------+------------+---------------+------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "|1  |25 |Private         |226802|11th        |7              |Never-married     |Machine-op-inspct|Own-child    |Black             |Male  |0           |0           |40            |United-States |<=50K |\n",
      "|2  |38 |Private         |89814 |HS-grad     |9              |Married-civ-spouse|Farming-fishing  |Husband      |White             |Male  |0           |0           |50            |United-States |<=50K |\n",
      "|3  |28 |Local-gov       |336951|Assoc-acdm  |12             |Married-civ-spouse|Protective-serv  |Husband      |White             |Male  |0           |0           |40            |United-States |>50K  |\n",
      "|4  |44 |Private         |160323|Some-college|10             |Married-civ-spouse|Machine-op-inspct|Husband      |Black             |Male  |7688        |0           |40            |United-States |>50K  |\n",
      "|5  |18 |?               |103497|Some-college|10             |Never-married     |?                |Own-child    |White             |Female|0           |0           |30            |United-States |<=50K |\n",
      "|6  |34 |Private         |198693|10th        |6              |Never-married     |Other-service    |Not-in-family|White             |Male  |0           |0           |30            |United-States |<=50K |\n",
      "|7  |29 |?               |227026|HS-grad     |9              |Never-married     |?                |Unmarried    |Black             |Male  |0           |0           |40            |United-States |<=50K |\n",
      "|8  |63 |Self-emp-not-inc|104626|Prof-school |15             |Married-civ-spouse|Prof-specialty   |Husband      |White             |Male  |3103        |0           |32            |United-States |>50K  |\n",
      "|9  |24 |Private         |369667|Some-college|10             |Never-married     |Other-service    |Unmarried    |White             |Female|0           |0           |40            |United-States |<=50K |\n",
      "|10 |55 |Private         |104996|7th-8th     |4              |Married-civ-spouse|Craft-repair     |Husband      |White             |Male  |0           |0           |10            |United-States |<=50K |\n",
      "|11 |65 |Private         |184454|HS-grad     |9              |Married-civ-spouse|Machine-op-inspct|Husband      |White             |Male  |6418        |0           |40            |United-States |>50K  |\n",
      "|12 |36 |Federal-gov     |212465|Bachelors   |13             |Married-civ-spouse|Adm-clerical     |Husband      |White             |Male  |0           |0           |40            |United-States |<=50K |\n",
      "|13 |26 |Private         |82091 |HS-grad     |9              |Never-married     |Adm-clerical     |Not-in-family|White             |Female|0           |0           |39            |United-States |<=50K |\n",
      "|14 |58 |?               |299831|HS-grad     |9              |Married-civ-spouse|?                |Husband      |White             |Male  |0           |0           |35            |United-States |<=50K |\n",
      "|15 |48 |Private         |279724|HS-grad     |9              |Married-civ-spouse|Machine-op-inspct|Husband      |White             |Male  |3103        |0           |48            |United-States |>50K  |\n",
      "|16 |43 |Private         |346189|Masters     |14             |Married-civ-spouse|Exec-managerial  |Husband      |White             |Male  |0           |0           |50            |United-States |>50K  |\n",
      "|17 |20 |State-gov       |444554|Some-college|10             |Never-married     |Other-service    |Own-child    |White             |Male  |0           |0           |25            |United-States |<=50K |\n",
      "|18 |43 |Private         |128354|HS-grad     |9              |Married-civ-spouse|Adm-clerical     |Wife         |White             |Female|0           |0           |30            |United-States |<=50K |\n",
      "|19 |37 |Private         |60548 |HS-grad     |9              |Widowed           |Machine-op-inspct|Unmarried    |White             |Female|0           |0           |20            |United-States |<=50K |\n",
      "|20 |40 |Private         |85019 |Doctorate   |16             |Married-civ-spouse|Prof-specialty   |Husband      |Asian-Pac-Islander|Male  |0           |0           |45            |?             |>50K  |\n",
      "+---+---+----------------+------+------------+---------------+------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning Example\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "from pyspark import SparkFiles\n",
    "sc = spark.sparkContext\n",
    "sc.addFile(url)\n",
    "dfAdultData = spark.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)\n",
    "dfAdultData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "df_string = convertColumn(dfAdultData, CONTI_FEATURES, FloatType())\n",
    "df_string.show(truncate=False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAdultData.select('age','fnlwgt').show(5)\n",
    "dfAdultData.describe().show()\n",
    "dfAdultData.crosstab('age', 'label').sort(\"age_label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAdultData.drop('education_num').columns\n",
    "age_square = df.select(col(\"age\")**2)\n",
    "dfAdultData = dfAdultData.withColumn(\"age_square\", col(\"age\")**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['age', 'age_square', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "           'hours_week', 'native_country', 'label']\n",
    "dfAdultData = dfAdultData.select(COLUMNS)\n",
    "dfAdultData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(dfAdultData)\n",
    "indexed = model.transform(dfAdultData)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Pipe line\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")stages += [assembler]\n",
    "ages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(dfAdultData)\n",
    "model = pipelineModel.transform(dfAdultData)\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))\n",
    "train_data, test_data = dfAdultData.randomSplit([.8,.2],seed=1234)\n",
    "train_data.groupby('label').agg({'label': 'count'}).show()\n",
    "test_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "predictions = linearModel.transform(test_data)\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
